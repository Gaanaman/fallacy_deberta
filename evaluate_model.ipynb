{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DeBERTa V2 Model Evaluation\n",
                "\n",
                "**Purpose**: Evaluate a pre-trained DeBERTa V2 XLarge model for fallacy detection\n",
                "\n",
                "**Features**:\n",
                "- Load saved model (no retraining required)\n",
                "- Generate predictions on test set\n",
                "- Classification report with per-class metrics\n",
                "- Normalized confusion matrix visualization\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from tqdm import tqdm\n",
                "\n",
                "# HuggingFace\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DebertaV2Tokenizer\n",
                "\n",
                "# Metrics\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix,\n",
                "    ConfusionMatrixDisplay,\n",
                "    classification_report,\n",
                "    accuracy_score,\n",
                "    precision_recall_fscore_support\n",
                ")\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"All libraries imported successfully!\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class EvalConfig:\n",
                "    \"\"\"\n",
                "    Configuration for model evaluation.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Model path (update this to your saved model location)\n",
                "    model_path: str = \"./best_model\"\n",
                "    \n",
                "    # Data paths\n",
                "    test_data_path: str = \"Data/fallacy_test.csv\"\n",
                "    val_data_path: str = \"Data/fallacy_val.csv\"\n",
                "    \n",
                "    # Output\n",
                "    output_dir: str = \"./output/evaluation\"\n",
                "    \n",
                "    # Model settings\n",
                "    max_seq_length: int = 512\n",
                "\n",
                "\n",
                "config = EvalConfig()\n",
                "\n",
                "# Create output directory\n",
                "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"EVALUATION CONFIGURATION\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nModel Path: {config.model_path}\")\n",
                "print(f\"Test Data: {config.test_data_path}\")\n",
                "print(f\"Validation Data: {config.val_data_path}\")\n",
                "print(f\"Output Directory: {config.output_dir}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Setup Device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"DEVICE CONFIGURATION\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "if torch.backends.mps.is_available():\n",
                "    device = torch.device(\"mps\")\n",
                "    device_name = \"Apple Silicon GPU (MPS)\"\n",
                "elif torch.cuda.is_available():\n",
                "    device = torch.device(\"cuda\")\n",
                "    device_name = f\"CUDA GPU: {torch.cuda.get_device_name(0)}\"\n",
                "else:\n",
                "    device = torch.device(\"cpu\")\n",
                "    device_name = \"CPU\"\n",
                "\n",
                "print(f\"\\nUsing: {device_name}\")\n",
                "print(f\"Device: {device}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"LOADING MODEL\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\\nLoading model from: {config.model_path}\")\n",
                "print(\"This may take a moment...\\n\")\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForSequenceClassification.from_pretrained(config.model_path)\n",
                "model.to(device)\n",
                "model.eval()\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = DebertaV2Tokenizer.from_pretrained(config.model_path)\n",
                "\n",
                "# Get label mappings\n",
                "id2label = model.config.id2label\n",
                "label2id = model.config.label2id\n",
                "num_labels = len(id2label)\n",
                "label_names = [id2label[i] for i in range(num_labels)]\n",
                "\n",
                "print(\"Model loaded successfully!\")\n",
                "print(f\"\\nModel Parameters: {model.num_parameters():,}\")\n",
                "print(f\"Number of Labels: {num_labels}\")\n",
                "print(f\"\\nFallacy Classes:\")\n",
                "for idx, label in id2label.items():\n",
                "    print(f\"  {idx}: {label}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Load Test Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"LOADING DATA\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Load test dataset\n",
                "test_dataset = load_dataset(\"csv\", data_files=config.test_data_path, split=\"train\")\n",
                "print(f\"\\nTest samples: {len(test_dataset)}\")\n",
                "\n",
                "# Load validation dataset\n",
                "val_dataset = load_dataset(\"csv\", data_files=config.val_data_path, split=\"train\")\n",
                "print(f\"Validation samples: {len(val_dataset)}\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Generate Predictions\n",
                "\n",
                "Run inference on the test set to generate predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_predictions(dataset, model, tokenizer, device, label2id):\n",
                "    \"\"\"\n",
                "    Generate predictions for a dataset.\n",
                "    \n",
                "    Args:\n",
                "        dataset: HuggingFace dataset\n",
                "        model: Trained model\n",
                "        tokenizer: Tokenizer\n",
                "        device: torch device\n",
                "        label2id: Label to ID mapping\n",
                "        \n",
                "    Returns:\n",
                "        Tuple of (true_labels, predictions, probabilities)\n",
                "    \"\"\"\n",
                "    true_labels = []\n",
                "    predictions = []\n",
                "    all_probs = []\n",
                "    \n",
                "    model.eval()\n",
                "    \n",
                "    for example in tqdm(dataset, desc=\"Generating predictions\"):\n",
                "        # Tokenize\n",
                "        inputs = tokenizer(\n",
                "            example[\"text\"],\n",
                "            return_tensors=\"pt\",\n",
                "            truncation=True,\n",
                "            max_length=config.max_seq_length\n",
                "        )\n",
                "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
                "        \n",
                "        # Predict\n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "            probs = torch.softmax(outputs.logits, dim=1)\n",
                "            pred = torch.argmax(probs, dim=1).item()\n",
                "        \n",
                "        predictions.append(pred)\n",
                "        true_labels.append(label2id[example[\"label\"]])\n",
                "        all_probs.append(probs.cpu().numpy()[0])\n",
                "    \n",
                "    return np.array(true_labels), np.array(predictions), np.array(all_probs)\n",
                "\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"GENERATING TEST PREDICTIONS\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "test_labels, test_preds, test_probs = get_predictions(\n",
                "    test_dataset, model, tokenizer, device, label2id\n",
                ")\n",
                "\n",
                "print(f\"\\nPredictions generated for {len(test_preds)} samples\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Classification Report (Test Set)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"CLASSIFICATION REPORT (Test Set)\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "# Calculate overall metrics\n",
                "accuracy = accuracy_score(test_labels, test_preds)\n",
                "precision, recall, f1, _ = precision_recall_fscore_support(\n",
                "    test_labels, test_preds, average='weighted', zero_division=0\n",
                ")\n",
                "\n",
                "print(f\"Overall Metrics:\")\n",
                "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
                "print(f\"  Precision: {precision:.4f}\")\n",
                "print(f\"  Recall:    {recall:.4f}\")\n",
                "print(f\"  F1 Score:  {f1:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"-\"*70)\n",
                "print(\"\\nPer-Class Metrics:\\n\")\n",
                "print(classification_report(\n",
                "    test_labels,\n",
                "    test_preds,\n",
                "    target_names=label_names,\n",
                "    zero_division=0\n",
                "))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Confusion Matrix (Test Set)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"CONFUSION MATRIX (Test Set)\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "# Create normalized confusion matrix\n",
                "cm = confusion_matrix(test_labels, test_preds, normalize='true')\n",
                "\n",
                "# Plot\n",
                "fig, ax = plt.subplots(figsize=(14, 12))\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
                "disp.plot(\n",
                "    cmap=\"Blues\",\n",
                "    values_format=\".2f\",\n",
                "    ax=ax,\n",
                "    colorbar=True,\n",
                "    xticks_rotation='vertical'\n",
                ")\n",
                "plt.title(\"Normalized Confusion Matrix (Test Set)\\nDeBERTa V2 XLarge - Fallacy Detection\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "\n",
                "# Save\n",
                "save_path = Path(config.output_dir) / \"confusion_matrix_test.png\"\n",
                "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "print(f\"Saved to: {save_path}\")\n",
                "\n",
                "plt.show()\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Validation Set Evaluation (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"GENERATING VALIDATION PREDICTIONS\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "val_labels, val_preds, val_probs = get_predictions(\n",
                "    val_dataset, model, tokenizer, device, label2id\n",
                ")\n",
                "\n",
                "# Calculate metrics\n",
                "val_accuracy = accuracy_score(val_labels, val_preds)\n",
                "val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
                "    val_labels, val_preds, average='weighted', zero_division=0\n",
                ")\n",
                "\n",
                "print(f\"\\nValidation Set Metrics:\")\n",
                "print(f\"  Accuracy:  {val_accuracy:.4f}\")\n",
                "print(f\"  Precision: {val_precision:.4f}\")\n",
                "print(f\"  Recall:    {val_recall:.4f}\")\n",
                "print(f\"  F1 Score:  {val_f1:.4f}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Save Evaluation Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Create evaluation report\n",
                "evaluation_report = {\n",
                "    \"model_path\": config.model_path,\n",
                "    \"test_results\": {\n",
                "        \"samples\": len(test_labels),\n",
                "        \"accuracy\": float(accuracy),\n",
                "        \"precision\": float(precision),\n",
                "        \"recall\": float(recall),\n",
                "        \"f1_score\": float(f1)\n",
                "    },\n",
                "    \"validation_results\": {\n",
                "        \"samples\": len(val_labels),\n",
                "        \"accuracy\": float(val_accuracy),\n",
                "        \"precision\": float(val_precision),\n",
                "        \"recall\": float(val_recall),\n",
                "        \"f1_score\": float(val_f1)\n",
                "    },\n",
                "    \"label_names\": label_names\n",
                "}\n",
                "\n",
                "# Save report\n",
                "report_path = Path(config.output_dir) / \"evaluation_report.json\"\n",
                "with open(report_path, \"w\") as f:\n",
                "    json.dump(evaluation_report, f, indent=2)\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"EVALUATION REPORT SAVED\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nReport saved to: {report_path}\")\n",
                "print(f\"\\nSummary:\")\n",
                "print(json.dumps(evaluation_report, indent=2))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Evaluation Complete!\n",
                "\n",
                "### Output Files\n",
                "\n",
                "Located in `./output/evaluation/`:\n",
                "- `confusion_matrix_test.png` - Visualization of model performance\n",
                "- `evaluation_report.json` - Complete metrics report\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. Analyze which fallacy pairs are commonly confused\n",
                "2. Review misclassified examples\n",
                "3. Compare with other model variants\n",
                "4. Use the TUI demo for interactive testing (`python fallacy_detector_tui.py`)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lab_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}