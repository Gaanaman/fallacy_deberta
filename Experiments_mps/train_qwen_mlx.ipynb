{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Qwen Model Training with MLX Framework\n",
                "\n",
                "**Purpose**: Fine-tune Qwen 2.5 14B model using MLX framework for logical fallacy detection\n",
                "\n",
                "**Dataset**: FLICC (Fallacy detection dataset with train/val/test splits)\n",
                "\n",
                "**Method**: LoRA (Low-Rank Adaptation) fine-tuning with Technocognitive Adaptation\n",
                "\n",
                "---\n",
                "\n",
                "## Configuration Overview\n",
                "\n",
                "- **Model**: Qwen/Qwen2.5-14B-Instruct\n",
                "- **LoRA Rank**: 16 (higher rank to compensate for LoRA weakness)\n",
                "- **LoRA Alpha**: 32 (2x rank for stability)\n",
                "- **Batch Size**: 1 (optimized for 32GB RAM)\n",
                "- **Gradient Accumulation**: 16 (effective batch size of 16)\n",
                "- **Learning Rate**: 1e-5 (paper-optimal)\n",
                "- **Iterations**: 1000 (~6 epochs over 2500 samples)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core libraries\n",
                "import json\n",
                "import argparse\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "# MLX framework\n",
                "import mlx.core as mx\n",
                "import mlx.nn as nn\n",
                "import mlx.optimizers as optim\n",
                "from mlx_lm import load, generate\n",
                "from mlx_lm import lora\n",
                "\n",
                "# Data processing\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Utilities\n",
                "from tqdm import tqdm\n",
                "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
                "\n",
                "print(\"All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Training Configuration\n",
                "\n",
                "### Technocognitive Adaptation Parameters\n",
                "\n",
                "Based on research findings:\n",
                "- **Rank 16**: Higher rank to compensate for LoRA weakness noted in paper\n",
                "- **Alpha 32**: Standard stability rule (Alpha = 2 × Rank)\n",
                "- **Learning Rate 1e-5**: Paper-validated optimal rate\n",
                "- **Scale 10.0**: Enhanced adaptation scaling\n",
                "- **Comprehensive Layer Coverage**: All projection layers for maximum adaptation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    \"\"\"\n",
                "    Training configuration based on Technocognitive Adaptation research.\n",
                "    Optimized for 32GB RAM with Qwen 2.5 14B model.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Base Model\n",
                "    model_name: str = \"Qwen/Qwen2.5-14B-Instruct\"\n",
                "    \n",
                "    # LoRA Configuration (Technocognitive Adaptation)\n",
                "    lora_rank: int = 16              # Higher rank to compensate for LoRA weakness\n",
                "    lora_alpha: int = 32             # Alpha = 2 * Rank (stability rule)\n",
                "    lora_dropout: float = 0.05       # Standard dropout for regularization\n",
                "    lora_scale: float = 10.0         # Enhanced adaptation scaling\n",
                "    \n",
                "    # LoRA target layers - comprehensive coverage\n",
                "    lora_layers: List[str] = None    # Will be set in __post_init__\n",
                "    \n",
                "    # Training Hyperparameters (32GB RAM optimized)\n",
                "    batch_size: int = 1                      # Required for 32GB RAM\n",
                "    gradient_accumulation_steps: int = 16    # Effective batch size = 16\n",
                "    learning_rate: float = 1.0e-5            # Paper-validated optimal rate\n",
                "    weight_decay: float = 0.01               # Weight decay for regularization\n",
                "    \n",
                "    # Training Duration\n",
                "    # Dataset ~2500 rows, 1000 iterations with batch 16 ≈ 6 epochs\n",
                "    iters: int = 1000                # Total training iterations\n",
                "    steps_per_eval: int = 100        # Validation frequency\n",
                "    save_every: int = 100            # Checkpoint frequency\n",
                "    steps_per_report: int = 10       # Loss reporting frequency\n",
                "    \n",
                "    # Data Paths\n",
                "    train_data_path: str = \"Data/fallacy_train.csv\"\n",
                "    val_data_path: str = \"Data/fallacy_val.csv\"\n",
                "    test_data_path: str = \"Data/fallacy_test.csv\"\n",
                "    \n",
                "    # Output Configuration\n",
                "    output_dir: str = \"./output/qwen14b_flicc\"\n",
                "    data_dir: str = \"./data\"\n",
                "    \n",
                "    # Training Options\n",
                "    max_seq_length: int = 512        # Maximum sequence length\n",
                "    seed: int = 42                   # Random seed for reproducibility\n",
                "    grad_checkpoint: bool = True     # Gradient checkpointing (saves memory)\n",
                "    val_batches: int = 25            # Validation batches per evaluation\n",
                "    \n",
                "    def __post_init__(self):\n",
                "        \"\"\"Initialize LoRA layers after dataclass creation.\"\"\"\n",
                "        if self.lora_layers is None:\n",
                "            # Comprehensive layer coverage as specified\n",
                "            self.lora_layers = [\n",
                "                \"q_proj\",      # Query projection\n",
                "                \"v_proj\",      # Value projection\n",
                "                \"k_proj\",      # Key projection\n",
                "                \"o_proj\",      # Output projection\n",
                "                \"gate_proj\",   # Gate projection (MLP)\n",
                "                \"up_proj\",     # Up projection (MLP)\n",
                "                \"down_proj\"    # Down projection (MLP)\n",
                "            ]\n",
                "\n",
                "\n",
                "# Initialize configuration\n",
                "config = TrainingConfig()\n",
                "\n",
                "# Display configuration\n",
                "print(\"=\"*70)\n",
                "print(\"TRAINING CONFIGURATION (Technocognitive Adaptation)\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nModel: {config.model_name}\")\n",
                "\n",
                "print(f\"\\nLoRA Settings (Technocognitive Adaptation):\")\n",
                "print(f\"  Rank: {config.lora_rank} (higher to compensate for LoRA weakness)\")\n",
                "print(f\"  Alpha: {config.lora_alpha} (2 × Rank stability rule)\")\n",
                "print(f\"  Dropout: {config.lora_dropout}\")\n",
                "print(f\"  Scale: {config.lora_scale}\")\n",
                "print(f\"  Target Layers: {', '.join(config.lora_layers)}\")\n",
                "\n",
                "print(f\"\\nTraining Settings (32GB RAM Optimized):\")\n",
                "print(f\"  Batch Size: {config.batch_size}\")\n",
                "print(f\"  Gradient Accumulation: {config.gradient_accumulation_steps}\")\n",
                "print(f\"  Effective Batch Size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
                "print(f\"  Learning Rate: {config.learning_rate} (paper-optimal)\")\n",
                "print(f\"  Weight Decay: {config.weight_decay}\")\n",
                "\n",
                "print(f\"\\nTraining Duration:\")\n",
                "print(f\"  Total Iterations: {config.iters}\")\n",
                "print(f\"  Approximate Epochs: ~6 (over ~2500 samples)\")\n",
                "print(f\"  Validation Every: {config.steps_per_eval} steps\")\n",
                "print(f\"  Save Checkpoint Every: {config.save_every} steps\")\n",
                "\n",
                "print(f\"\\nData Splits:\")\n",
                "print(f\"  Training: {config.train_data_path}\")\n",
                "print(f\"  Validation: {config.val_data_path}\")\n",
                "print(f\"  Test: {config.test_data_path}\")\n",
                "\n",
                "print(f\"\\nOutput Directory: {config.output_dir}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Data Loading and Preparation\n",
                "\n",
                "Load all three data splits: train, validation, and test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_fallacy_data(file_path: str, split_name: str = \"data\") -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load fallacy detection dataset from CSV file.\n",
                "    \n",
                "    Args:\n",
                "        file_path: Path to the CSV file\n",
                "        split_name: Name of the split (train/val/test) for display\n",
                "        \n",
                "    Returns:\n",
                "        DataFrame with 'text' and 'label' columns\n",
                "    \"\"\"\n",
                "    print(f\"\\nLoading {split_name} data from: {file_path}\")\n",
                "    \n",
                "    try:\n",
                "        df = pd.read_csv(file_path)\n",
                "        print(f\"Loaded {len(df):,} examples\")\n",
                "        \n",
                "        # Display label distribution\n",
                "        print(f\"\\nLabel Distribution ({split_name}):\")\n",
                "        label_counts = df['label'].value_counts().sort_index()\n",
                "        for label, count in label_counts.items():\n",
                "            print(f\"  {label:25s}: {count:4d} ({count/len(df)*100:.1f}%)\")\n",
                "        \n",
                "        return df\n",
                "        \n",
                "    except FileNotFoundError:\n",
                "        print(f\"ERROR: File not found at {file_path}\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        print(f\"ERROR loading data: {e}\")\n",
                "        raise\n",
                "\n",
                "\n",
                "# Load all three data splits\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"LOADING ALL DATA SPLITS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "train_df = load_fallacy_data(config.train_data_path, \"TRAIN\")\n",
                "val_df = load_fallacy_data(config.val_data_path, \"VALIDATION\")\n",
                "test_df = load_fallacy_data(config.test_data_path, \"TEST\")\n",
                "\n",
                "print(f\"\\n\" + \"=\"*70)\n",
                "print(\"DATA LOADING SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Training samples:   {len(train_df):,}\")\n",
                "print(f\"Validation samples: {len(val_df):,}\")\n",
                "print(f\"Test samples:       {len(test_df):,}\")\n",
                "print(f\"Total samples:      {len(train_df) + len(val_df) + len(test_df):,}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Prompt Engineering\n",
                "\n",
                "Create prompts in Qwen's chat format for instruction fine-tuning.\n",
                "\n",
                "**Format**:\n",
                "```\n",
                "<|im_start|>system\n",
                "[System instructions]\n",
                "<|im_end|>\n",
                "<|im_start|>user\n",
                "[User query]\n",
                "<|im_end|>\n",
                "<|im_start|>assistant\n",
                "[Model response]\n",
                "<|im_end|>\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define fallacy categories\n",
                "FALLACY_CATEGORIES = [\n",
                "    \"ad hominem\",\n",
                "    \"anecdote\",\n",
                "    \"cherry picking\",\n",
                "    \"conspiracy theory\",\n",
                "    \"fake experts\",\n",
                "    \"false choice\",\n",
                "    \"false equivalence\",\n",
                "    \"impossible expectations\",\n",
                "    \"misrepresentation\",\n",
                "    \"oversimplification\",\n",
                "    \"single cause\",\n",
                "    \"slothful induction\"\n",
                "]\n",
                "\n",
                "def prepare_prompt(text: str, label: Optional[str] = None) -> str:\n",
                "    \"\"\"\n",
                "    Prepare prompt in Qwen chat format.\n",
                "    \n",
                "    Args:\n",
                "        text: Input text to classify\n",
                "        label: Ground truth label (for training) or None (for inference)\n",
                "        \n",
                "    Returns:\n",
                "        Formatted prompt string\n",
                "    \"\"\"\n",
                "    # System instruction\n",
                "    system_msg = (\n",
                "        \"You are a logical fallacy detection expert. \"\n",
                "        \"Classify the given text into one of these fallacy categories: \"\n",
                "        f\"{', '.join(FALLACY_CATEGORIES)}. \"\n",
                "        \"Respond with only the fallacy category name.\"\n",
                "    )\n",
                "    \n",
                "    # User query\n",
                "    user_msg = f\"Text: {text}\\n\\nWhat type of fallacy is present in this text?\"\n",
                "    \n",
                "    # Build prompt\n",
                "    if label is not None:\n",
                "        # Training format (includes the answer)\n",
                "        prompt = (\n",
                "            f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n\"\n",
                "            f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
                "            f\"<|im_start|>assistant\\n{label}<|im_end|>\"\n",
                "        )\n",
                "    else:\n",
                "        # Inference format (no answer)\n",
                "        prompt = (\n",
                "            f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n\"\n",
                "            f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
                "            f\"<|im_start|>assistant\\n\"\n",
                "        )\n",
                "    \n",
                "    return prompt\n",
                "\n",
                "\n",
                "def prepare_dataset(df: pd.DataFrame, split_name: str = \"data\") -> List[Dict[str, str]]:\n",
                "    \"\"\"\n",
                "    Convert DataFrame to MLX training format (JSONL).\n",
                "    \n",
                "    Args:\n",
                "        df: DataFrame with 'text' and 'label' columns\n",
                "        split_name: Name of the split for display\n",
                "        \n",
                "    Returns:\n",
                "        List of dictionaries with 'text' key containing formatted prompts\n",
                "    \"\"\"\n",
                "    print(f\"\\nPreparing {split_name} dataset: {len(df):,} examples...\")\n",
                "    \n",
                "    dataset = []\n",
                "    for idx, row in df.iterrows():\n",
                "        prompt = prepare_prompt(row['text'], row['label'])\n",
                "        dataset.append({\"text\": prompt})\n",
                "        \n",
                "        # Show progress every 500 examples\n",
                "        if (idx + 1) % 500 == 0:\n",
                "            print(f\"  Processed {idx + 1:,}/{len(df):,} examples...\")\n",
                "    \n",
                "    print(f\"Preparation complete for {split_name}!\")\n",
                "    return dataset\n",
                "\n",
                "\n",
                "# Example prompt display\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"EXAMPLE PROMPT FORMAT\")\n",
                "print(\"=\"*70)\n",
                "sample_text = train_df.iloc[0]['text']\n",
                "sample_label = train_df.iloc[0]['label']\n",
                "example_prompt = prepare_prompt(sample_text, sample_label)\n",
                "print(example_prompt[:500] + \"...\" if len(example_prompt) > 500 else example_prompt)\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Prepare all datasets\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"PREPARING DATASETS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "train_dataset = prepare_dataset(train_df, \"TRAIN\")\n",
                "val_dataset = prepare_dataset(val_df, \"VALIDATION\")\n",
                "test_dataset = prepare_dataset(test_df, \"TEST\")\n",
                "\n",
                "print(f\"\\n\" + \"=\"*70)\n",
                "print(\"DATASET PREPARATION SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Training examples:   {len(train_dataset):,}\")\n",
                "print(f\"Validation examples: {len(val_dataset):,}\")\n",
                "print(f\"Test examples:       {len(test_dataset):,}\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Save Datasets in JSONL Format\n",
                "\n",
                "MLX requires data in JSONL (JSON Lines) format. We'll save train and validation sets.\n",
                "The test set will be used later for final evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data directory\n",
                "data_dir = Path(config.data_dir)\n",
                "data_dir.mkdir(exist_ok=True)\n",
                "\n",
                "print(\"\\nSaving datasets to JSONL format...\")\n",
                "\n",
                "# Save training data\n",
                "train_path = data_dir / \"train.jsonl\"\n",
                "with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    for item in train_dataset:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
                "print(f\"Saved training data to: {train_path}\")\n",
                "\n",
                "# Save validation data\n",
                "val_path = data_dir / \"valid.jsonl\"\n",
                "with open(val_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    for item in val_dataset:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
                "print(f\"Saved validation data to: {val_path}\")\n",
                "\n",
                "# Note: Test set is kept separate for final evaluation after training\n",
                "print(f\"\\nNote: Test set ({len(test_dataset):,} examples) reserved for final evaluation\")\n",
                "print(\"\\nAll datasets saved successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Setup Training Arguments\n",
                "\n",
                "Configure MLX-LM trainer with Technocognitive Adaptation parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare training arguments with Technocognitive Adaptation settings\n",
                "train_args = argparse.Namespace(\n",
                "    # Model and data\n",
                "    model=config.model_name,\n",
                "    data=str(data_dir),\n",
                "    train=True,\n",
                "    \n",
                "    # LoRA configuration (Technocognitive Adaptation)\n",
                "    lora_layers=16,  # Number of layers to apply LoRA (will use the keys defined)\n",
                "    \n",
                "    # Training hyperparameters\n",
                "    batch_size=config.batch_size,\n",
                "    iters=config.iters,\n",
                "    val_batches=config.val_batches,\n",
                "    learning_rate=config.learning_rate,\n",
                "    \n",
                "    # Evaluation and reporting\n",
                "    steps_per_report=config.steps_per_report,\n",
                "    steps_per_eval=config.steps_per_eval,\n",
                "    save_every=config.save_every,\n",
                "    \n",
                "    # Output and checkpointing\n",
                "    adapter_path=config.output_dir,\n",
                "    \n",
                "    # Advanced options\n",
                "    grad_checkpoint=config.grad_checkpoint,\n",
                "    seed=config.seed,\n",
                "    use_dora=False,\n",
                "    resume_adapter_file=None,\n",
                "    \n",
                "    # Sequence length\n",
                "    max_seq_length=config.max_seq_length,\n",
                "    \n",
                "    # Test settings (disabled during training)\n",
                "    test=False,\n",
                "    test_batches=0\n",
                ")\n",
                "\n",
                "# Display training plan\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"TRAINING PLAN (Technocognitive Adaptation)\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nData:\")\n",
                "print(f\"  Training examples: {len(train_dataset):,}\")\n",
                "print(f\"  Validation examples: {len(val_dataset):,}\")\n",
                "\n",
                "print(f\"\\nBatching:\")\n",
                "print(f\"  Batch size: {config.batch_size}\")\n",
                "print(f\"  Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
                "print(f\"  Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
                "\n",
                "print(f\"\\nTraining Schedule:\")\n",
                "print(f\"  Total iterations: {config.iters:,}\")\n",
                "print(f\"  Approximate epochs: ~6\")\n",
                "print(f\"  Samples per iteration: {config.batch_size * config.gradient_accumulation_steps}\")\n",
                "print(f\"  Total samples processed: ~{config.iters * config.batch_size * config.gradient_accumulation_steps:,}\")\n",
                "\n",
                "print(f\"\\nEvaluation & Checkpointing:\")\n",
                "print(f\"  Validate every: {config.steps_per_eval} steps\")\n",
                "print(f\"  Save checkpoint every: {config.save_every} steps\")\n",
                "print(f\"  Report metrics every: {config.steps_per_report} steps\")\n",
                "print(f\"  Total checkpoints: ~{config.iters // config.save_every}\")\n",
                "\n",
                "print(f\"\\nOptimization:\")\n",
                "print(f\"  Learning rate: {config.learning_rate} (paper-optimal)\")\n",
                "print(f\"  Weight decay: {config.weight_decay}\")\n",
                "print(f\"  LoRA rank: {config.lora_rank}\")\n",
                "print(f\"  LoRA alpha: {config.lora_alpha}\")\n",
                "print(f\"  LoRA scale: {config.lora_scale}\")\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Start Training\n",
                "\n",
                "**Important**: Training the 14B model will take several hours.\n",
                "\n",
                "**Expected Duration**: 1000 iterations with validation and checkpointing may take 4-8 hours depending on hardware.\n",
                "\n",
                "Monitor the output for:\n",
                "- Training loss (should decrease steadily)\n",
                "- Validation loss (should decrease, watch for overfitting)\n",
                "- Checkpoints (saved every 100 steps)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"STARTING TRAINING\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nModel: {config.model_name}\")\n",
                "print(f\"Output: {config.output_dir}\")\n",
                "print(f\"\\nExpected duration: 4-8 hours (depends on hardware)\")\n",
                "print(f\"\\nThe model will be saved at:\")\n",
                "print(f\"  - Every {config.save_every} steps\")\n",
                "print(f\"  - After training completes\")\n",
                "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
                "\n",
                "# Load base model\n",
                "print(\"Loading base model (this may take a few minutes)...\")\n",
                "model, tokenizer = load(config.model_name)\n",
                "print(\"Model loaded successfully!\\n\")\n",
                "\n",
                "# Start training\n",
                "print(\"Training in progress...\")\n",
                "print(\"Monitor the logs below for training progress.\\n\")\n",
                "print(\"-\" * 70)\n",
                "\n",
                "lora.train(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    args=train_args\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"-\" * 70)\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nModel adapter saved to: {config.output_dir}\")\n",
                "print(f\"\\nYou can now:\")\n",
                "print(f\"  1. Load the fine-tuned model for inference\")\n",
                "print(f\"  2. Evaluate on the test set\")\n",
                "print(f\"  3. Deploy the model\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Load the Fine-Tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nLoading fine-tuned model with trained adapter...\")\n",
                "\n",
                "# Load model with LoRA adapter\n",
                "model, tokenizer = load(\n",
                "    config.model_name,\n",
                "    adapter_path=config.output_dir\n",
                ")\n",
                "\n",
                "print(\"Model loaded successfully!\")\n",
                "print(f\"Using adapter from: {config.output_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Quick Test with Sample Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with sample examples\n",
                "test_examples = [\n",
                "    {\n",
                "        \"text\": \"Scientists say climate change is fake because they're all funded by the government.\",\n",
                "        \"expected\": \"ad hominem\"\n",
                "    },\n",
                "    {\n",
                "        \"text\": \"Vaccines are dangerous because my friend's cousin got sick after getting vaccinated.\",\n",
                "        \"expected\": \"anecdote\"\n",
                "    },\n",
                "    {\n",
                "        \"text\": \"The economy is doing great because the stock market is up.\",\n",
                "        \"expected\": \"cherry picking\"\n",
                "    }\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"QUICK MODEL TEST\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "for i, example in enumerate(test_examples, 1):\n",
                "    print(f\"Test {i}/{len(test_examples)}\")\n",
                "    print(\"-\" * 70)\n",
                "    print(f\"Input: {example['text']}\")\n",
                "    print(f\"Expected: {example['expected']}\")\n",
                "    \n",
                "    # Generate prediction\n",
                "    prompt = prepare_prompt(example['text'])\n",
                "    response = generate(\n",
                "        model,\n",
                "        tokenizer,\n",
                "        prompt=prompt,\n",
                "        max_tokens=20,\n",
                "        temp=0.1\n",
                "    )\n",
                "    \n",
                "    print(f\"Predicted: {response.strip()}\")\n",
                "    print()\n",
                "\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Validation Set Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(\n",
                "    model, \n",
                "    tokenizer, \n",
                "    df: pd.DataFrame,\n",
                "    split_name: str = \"Validation\",\n",
                "    num_samples: Optional[int] = None\n",
                ") -> tuple:\n",
                "    \"\"\"\n",
                "    Evaluate model on a dataset.\n",
                "    \n",
                "    Args:\n",
                "        model: Trained model\n",
                "        tokenizer: Model tokenizer\n",
                "        df: DataFrame to evaluate on\n",
                "        split_name: Name of the split for display\n",
                "        num_samples: Number of samples to evaluate (None = all)\n",
                "        \n",
                "    Returns:\n",
                "        Tuple of (predictions, true_labels)\n",
                "    \"\"\"\n",
                "    if num_samples is not None:\n",
                "        sample_df = df.sample(min(num_samples, len(df)), random_state=42)\n",
                "        print(f\"\\nEvaluating on {len(sample_df)} {split_name} samples...\\n\")\n",
                "    else:\n",
                "        sample_df = df\n",
                "        print(f\"\\nEvaluating on all {len(sample_df)} {split_name} samples...\\n\")\n",
                "    \n",
                "    predictions = []\n",
                "    true_labels = []\n",
                "    \n",
                "    # Evaluate with progress bar\n",
                "    for idx, row in tqdm(\n",
                "        sample_df.iterrows(), \n",
                "        total=len(sample_df),\n",
                "        desc=f\"Evaluating {split_name}\"\n",
                "    ):\n",
                "        prompt = prepare_prompt(row['text'])\n",
                "        \n",
                "        response = generate(\n",
                "            model,\n",
                "            tokenizer,\n",
                "            prompt=prompt,\n",
                "            max_tokens=20,\n",
                "            temp=0.1\n",
                "        )\n",
                "        \n",
                "        # Extract predicted label\n",
                "        pred_label = response.strip().split('\\n')[0].strip().lower()\n",
                "        true_label = row['label'].lower()\n",
                "        \n",
                "        predictions.append(pred_label)\n",
                "        true_labels.append(true_label)\n",
                "    \n",
                "    return predictions, true_labels\n",
                "\n",
                "\n",
                "# Evaluate on validation set\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"VALIDATION SET EVALUATION\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "val_predictions, val_true_labels = evaluate_model(\n",
                "    model, \n",
                "    tokenizer, \n",
                "    val_df,\n",
                "    split_name=\"Validation\",\n",
                "    num_samples=None  # Evaluate on all validation samples\n",
                ")\n",
                "\n",
                "# Calculate metrics\n",
                "val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"VALIDATION RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nAccuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(classification_report(val_true_labels, val_predictions))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 11: Test Set Evaluation (Final Performance)\n",
                "\n",
                "Evaluate on the held-out test set for final performance metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"TEST SET EVALUATION (Final Performance)\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "test_predictions, test_true_labels = evaluate_model(\n",
                "    model, \n",
                "    tokenizer, \n",
                "    test_df,\n",
                "    split_name=\"Test\",\n",
                "    num_samples=None  # Evaluate on all test samples\n",
                ")\n",
                "\n",
                "# Calculate metrics\n",
                "test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"FINAL TEST RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"\\nTest Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(classification_report(test_true_labels, test_predictions))\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(confusion_matrix(test_true_labels, test_predictions))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 12: Save Complete Training Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare comprehensive training report\n",
                "training_report = {\n",
                "    \"model_configuration\": {\n",
                "        \"model_name\": config.model_name,\n",
                "        \"lora_parameters\": {\n",
                "            \"rank\": config.lora_rank,\n",
                "            \"alpha\": config.lora_alpha,\n",
                "            \"dropout\": config.lora_dropout,\n",
                "            \"scale\": config.lora_scale,\n",
                "            \"target_layers\": config.lora_layers\n",
                "        }\n",
                "    },\n",
                "    \"training_parameters\": {\n",
                "        \"batch_size\": config.batch_size,\n",
                "        \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
                "        \"effective_batch_size\": config.batch_size * config.gradient_accumulation_steps,\n",
                "        \"learning_rate\": config.learning_rate,\n",
                "        \"weight_decay\": config.weight_decay,\n",
                "        \"total_iterations\": config.iters,\n",
                "        \"max_seq_length\": config.max_seq_length,\n",
                "        \"seed\": config.seed\n",
                "    },\n",
                "    \"data_splits\": {\n",
                "        \"train_samples\": len(train_dataset),\n",
                "        \"validation_samples\": len(val_dataset),\n",
                "        \"test_samples\": len(test_dataset)\n",
                "    },\n",
                "    \"evaluation_results\": {\n",
                "        \"validation_accuracy\": float(val_accuracy),\n",
                "        \"test_accuracy\": float(test_accuracy),\n",
                "        \"num_validation_evaluated\": len(val_predictions),\n",
                "        \"num_test_evaluated\": len(test_predictions)\n",
                "    },\n",
                "    \"fallacy_categories\": FALLACY_CATEGORIES\n",
                "}\n",
                "\n",
                "# Save training report\n",
                "report_path = Path(config.output_dir) / \"training_report.json\"\n",
                "with open(report_path, \"w\") as f:\n",
                "    json.dump(training_report, f, indent=2)\n",
                "\n",
                "print(\"\\nTraining report saved!\")\n",
                "print(f\"Location: {report_path}\")\n",
                "\n",
                "# Display summary\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"TRAINING SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "print(json.dumps(training_report, indent=2))\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Complete!\n",
                "\n",
                "### Summary\n",
                "\n",
                "Your Qwen 2.5 14B model has been successfully fine-tuned for fallacy detection using Technocognitive Adaptation parameters.\n",
                "\n",
                "### Output Files\n",
                "\n",
                "Located in `./output/qwen14b_flicc/`:\n",
                "- `adapters.safetensors` - Trained LoRA adapter weights\n",
                "- `adapter_config.json` - LoRA configuration\n",
                "- `training_report.json` - Complete training report with metrics\n",
                "\n",
                "### Performance Metrics\n",
                "\n",
                "- **Validation Accuracy**: See Step 10 results\n",
                "- **Test Accuracy**: See Step 11 results (final unbiased performance)\n",
                "\n",
                "### Usage Example\n",
                "\n",
                "```python\n",
                "from mlx_lm import load, generate\n",
                "\n",
                "# Load fine-tuned model\n",
                "model, tokenizer = load(\n",
                "    \"Qwen/Qwen2.5-14B-Instruct\",\n",
                "    adapter_path=\"./output/qwen14b_flicc\"\n",
                ")\n",
                "\n",
                "# Prepare prompt\n",
                "prompt = prepare_prompt(\"Your text here\")\n",
                "\n",
                "# Generate prediction\n",
                "response = generate(model, tokenizer, prompt=prompt, max_tokens=20, temp=0.1)\n",
                "print(f\"Predicted fallacy: {response.strip()}\")\n",
                "```\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. Analyze confusion matrix to identify which fallacies are confused\n",
                "2. Review misclassified examples for insights\n",
                "3. Consider additional fine-tuning if needed\n",
                "4. Deploy model for production use"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}